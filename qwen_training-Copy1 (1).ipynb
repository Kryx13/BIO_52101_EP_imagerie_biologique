{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eca8e7c-3e17-4b7b-b818-0b664ccb4b8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.49.0\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers==4.49.0)\n",
      "  Downloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.49.0)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.49.0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.49.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.49.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.49.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.49.0) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m207.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.31.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.49.0\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.6.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.21.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio==2.6.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (4.13.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m268.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m272.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m246.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m144.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m163.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m249.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m208.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision==0.21.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision==0.21.0) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (768.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m768.4/768.4 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m205.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.2\n",
      "    Uninstalling torchvision-0.17.2:\n",
      "      Successfully uninstalled torchvision-0.17.2\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.2.2\n",
      "    Uninstalling torchaudio-2.2.2:\n",
      "      Successfully uninstalled torchaudio-2.2.2\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting xformers==0.0.27.post2\n",
      "  Downloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl==0.15.2\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: triton in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.2.0)\n",
      "Collecting cut_cross_entropy\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2025.5.7-py3-none-any.whl.metadata (8.0 kB)\n",
      "Downloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m202.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m197.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading unsloth_zoo-2025.5.7-py3-none-any.whl (138 kB)\n",
      "Installing collected packages: xformers, unsloth_zoo, trl, peft, cut_cross_entropy, bitsandbytes, accelerate\n",
      "Successfully installed accelerate-1.7.0 bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 peft-0.15.2 trl-0.15.2 unsloth_zoo-2025.5.7 xformers-0.0.27.post2\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (5.29.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.31.2)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.13.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, propcache, multidict, hf_transfer, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2025.5.7 requires msgspec, which is not installed.\n",
      "unsloth-zoo 2025.5.7 requires tyro, which is not installed.\n",
      "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\n",
      "unsloth-zoo 2025.5.7 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
      "unsloth-zoo 2025.5.7 requires transformers!=4.47.0,==4.51.3, but you have transformers 4.49.0 which is incompatible.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.6.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2025.3.0 hf_transfer-0.1.9 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 sentencepiece-0.2.0 xxhash-3.5.0 yarl-1.20.0\n",
      "Collecting unsloth==2025.4.1\n",
      "  Downloading unsloth-2025.4.1-py3-none-any.whl.metadata (46 kB)\n",
      "Downloading unsloth-2025.4.1-py3-none-any.whl (193 kB)\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2025.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.49.0\n",
    "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.27.post2 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth==2025.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8a2548-bb0a-482f-a130-305beef592f6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfiglet\n",
      "  Downloading pyfiglet-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pyfiglet-1.0.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyfiglet\n",
      "Successfully installed pyfiglet-1.0.2\n"
     ]
    }
   ],
   "source": [
    "#!unzip VLM_GRPO-main.zip\n",
    "#!pip install -e VLM_GRPO-main\n",
    "#!pip install pyfiglet\n",
    "#!pip install -r VLM_GRPO-main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbe45d9-b991-4d26-96ac-1c03314a3895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "洶･ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      " _    ____    __  ___      __________  ____  ____         \n",
      "| |  / / /   /  |/  /     / ____/ __ \\/ __ \\/ __ \\        \n",
      "| | / / /   / /|_/ /_____/ / __/ /_/ / /_/ / / / /  ______\n",
      "| |/ / /___/ /  / /_____/ /_/ / _, _/ ____/ /_/ /  /_____/\n",
      "|___/_____/_/  /_/      \\____/_/ |_/_/    \\____/          \n",
      "                                                          \n",
      "    ____  ___  ______________  _______   ________\n",
      "   / __ \\/   |/_  __/ ____/ / / /  _/ | / / ____/\n",
      "  / /_/ / /| | / / / /   / /_/ // //  |/ / / __  \n",
      " / ____/ ___ |/ / / /___/ __  // // /|  / /_/ /  \n",
      "/_/   /_/  |_/_/  \\____/_/ /_/___/_/ |_/\\____/   \n",
      "                                                 \n",
      "   __  ___   _______ __    ____  ________  __\n",
      "  / / / / | / / ___// /   / __ \\/_  __/ / / /\n",
      " / / / /  |/ /\\__ \\/ /   / / / / / / / /_/ / \n",
      "/ /_/ / /|  /___/ / /___/ /_/ / / / / __  /  \n",
      "\\____/_/ |_//____/_____/\\____/ /_/ /_/ /_/   \n",
      "                                             \n",
      "\n",
      "Unsloth patched for VLMs GRPO training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.4.0+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.10.14 (you have 3.10.14)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "洶･ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.1: Fast Qwen2_5_Vl patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 21.951 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import vlmgrpo\n",
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "\n",
    "SFT_path=\"V2_GRPO\"\n",
    "#SFT_path=None\n",
    "if SFT_path != None :\n",
    "  model, tokenizer = FastVisionModel.from_pretrained(\n",
    "      SFT_path,\n",
    "      load_in_4bit=True,\n",
    "  )\n",
    "  #decommenter pour entrainer des nouveaux lora\n",
    "  #model=model.merge_and_unload()\n",
    "\n",
    "else:\n",
    "  model, tokenizer = FastVisionModel.from_pretrained(\n",
    "      \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n",
    "      load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "      use_gradient_checkpointing = False, # True or \"unsloth\" for long context\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a53dbbd-0503-4b76-93ef-aac4322cf6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = False, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071855ba-6e04-40fe-986c-1aaf4f7b223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os \n",
    "import pickle\n",
    "df = pd.read_csv('metadata_global.csv')\n",
    "\n",
    "with open('data.pkl','rb') as f:\n",
    "    dataset_CoT=pickle.load(f)\n",
    "\n",
    "df_CoT=pd.DataFrame(dataset_CoT)\n",
    "df_merged = pd.merge(df, df_CoT, left_on='isic_id', right_on='image_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4382841e-1701-4aa7-a749-865ab7f999c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = \"<think>\"\n",
    "reasoning_end   = \"</think>\"\n",
    "solution_start  = \"<answer>\"\n",
    "solution_end    = \"</answer>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem. Think about the problem and provide your working out. Place it between {reasoning_start} and {reasoning_end}. Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "prompt1 = \"\"\"Instruction : You are a medical image analysis assistant specialized in dermatology. You are shown a high-resolution image of a skin lesion.\n",
    "\n",
    "Your task is to carefully examine the image and provide a **detailed reasoning** based on observable features such as shape, color, border irregularities, texture, pattern...\n",
    "\n",
    "After your reasoning, provide the **final diagnosis** by selecting **only one** of the following predefined categories:\n",
    "\n",
    "Benign melanocytic proliferations\n",
    "Benign - Other\n",
    "Malignant melanocytic proliferations (Melanoma)\n",
    "Malignant adnexal epithelial proliferations - Follicular\n",
    "Benign epidermal proliferations\n",
    "Indeterminate epidermal proliferations\n",
    "Indeterminate melanocytic proliferations\n",
    "Collision - Only benign proliferations\n",
    "Malignant epidermal proliferations\n",
    "Benign soft tissue proliferations - Fibro-histiocytic\n",
    "Benign soft tissue proliferations - Vascular\n",
    "Collision - At least one malignant proliferation\n",
    "Flat melanotic pigmentations - not melanocytic nevus\n",
    "Inflammatory or infectious diseases\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1bc98b6-727b-4e14-9979-41d51557a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def img_aug(img):\n",
    "  transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomRotation(degrees=10, fill=(0, 0, 0)),\n",
    "  ])\n",
    "\n",
    "  return transform(img)\n",
    "\n",
    "\n",
    "def process_sample(item):\n",
    "    image_id = item[\"isic_id\"]\n",
    "    skin_type = item[\"type\"]\n",
    "    image_path = f\"dataset/{skin_type}/{image_id}.jpg\"\n",
    "    image = Image.open(image_path)\n",
    "    metadata = {'age_approx':item['age_approx'],'anatom_site_general':item['anatom_site_general'],'sex':item['sex']}\n",
    "    if pd.isna(item['diagnosis_2']):\n",
    "        diagnosis = item['diagnosis_1']\n",
    "    else:\n",
    "        diagnosis = item['diagnosis_2'] \n",
    "        \n",
    "    content = [{\"type\": \"text\",\"text\": prompt1 + f\"you are provided with an image andthe metadata concerning the patient:\\n approximate age: {metadata['age_approx']}, lesion location: {metadata['anatom_site_general']}, patient sex: {metadata['sex']}. Be concise.\"\n",
    "}]\n",
    "    content.append({\"type\": \"image\", \"image\": img_aug(image).resize((224,224))})\n",
    "    output =  {\"messages\": [\n",
    "            {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": content\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"text\", \"text\": item[\"CoT\"]}\n",
    "              ]\n",
    "          }\n",
    "      ]\n",
    "  }\n",
    "    return output\n",
    "\n",
    "data_list = df_merged.to_dict(orient=\"records\")\n",
    "\n",
    "train_dataset=data_list[:280]\n",
    "eval_dataset=data_list[280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f482cada-9421-4f9b-a913-3c8c3efb5f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 280 | Num Epochs = 10 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 14,024,704/7,000,000,000 (0.20% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/80 23:12 < 07:28, 0.04 it/s, Epoch 6.69/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.635900</td>\n",
       "      <td>1.586091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.425027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.231422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth import FastVisionModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "import os\n",
    "num_workers=os.cpu_count()-1\n",
    "from transformers import EarlyStoppingCallback\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # Optional set GPU device ID\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "config = SFTConfig(\n",
    "        dataloader_num_workers = num_workers,\n",
    "        per_device_train_batch_size = 8 ,\n",
    "        per_device_eval_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        #max_steps = 1000,\n",
    "        num_train_epochs = 10, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_SFT\",\n",
    "        report_to = \"none\",\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"messages\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "\n",
    "\n",
    "        eval_steps = 20,\n",
    "        eval_strategy = \"steps\",\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 40,\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "trainer = SFTTrainer(\n",
    "    \n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model,tokenizer,formatting_func=process_sample),\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    args = config\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb6694d-5abe-44c5-a429-fd85c40ed212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"V1_SFT\")  # Local saving\n",
    "tokenizer.save_pretrained(\"V1_SFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4c9b3b-e132-4c41-9b51-cdd24e387d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 'ISIC_0085644',\n",
       " 'prompt': [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Instruction : You are a medical image analysis assistant specialized in dermatology. You are shown a high-resolution image of a skin lesion.\\n\\nYour task is to carefully examine the image and provide a **detailed reasoning** based on observable features such as shape, color, border irregularities, texture, pattern...\\n\\nAfter your reasoning, provide the **final diagnosis** by selecting **only one** of the following predefined categories:\\n\\nBenign melanocytic proliferations\\nBenign - Other\\nMalignant melanocytic proliferations (Melanoma)\\nMalignant adnexal epithelial proliferations - Follicular\\nBenign epidermal proliferations\\nIndeterminate epidermal proliferations\\nIndeterminate melanocytic proliferations\\nCollision - Only benign proliferations\\nMalignant epidermal proliferations\\nBenign soft tissue proliferations - Fibro-histiocytic\\nBenign soft tissue proliferations - Vascular\\nCollision - At least one malignant proliferation\\nFlat melanotic pigmentations - not melanocytic nevus\\nInflammatory or infectious diseasesInput an image and the metadata concerning the patient:\\n approximate age: 50.0, lesion location: lower extremity, patient sex: femaleYou are given a problem. Think about the problem and provide your working out. Place it between <think> and </think>. Then, provide your solution between <answer></answer>\\n Be concise and short'},\n",
       "    {'type': 'image'}]}],\n",
       " 'image': [<PIL.Image.Image image mode=RGB size=224x224>],\n",
       " 'answer': 'Benign melanocytic proliferations'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis=['diagnosis_1', 'diagnosis_2', 'diagnosis_3','diagnosis_4', 'diagnosis_5']\n",
    "\n",
    "class FormattedDataset():\n",
    "    def __init__(self, dataset, format_fn=None):\n",
    "        self.dataset = dataset\n",
    "        self.format_fn = format_fn\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset.iloc[idx]\n",
    "        output = self.format_fn(item)\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "\n",
    "def img_aug(img):\n",
    "    pass\n",
    "def process_sample(item):\n",
    "    image_id = item[\"isic_id\"]\n",
    "    skin_type = item[\"type\"]\n",
    "    image_path = f\"dataset/{skin_type}/{image_id}.jpg\"\n",
    "    image = Image.open(image_path)\n",
    "    metadata = {'age_approx':item['age_approx'],'anatom_site_general':item['anatom_site_general'],'sex':item['sex']}\n",
    "\n",
    "    if pd.isna(item['diagnosis_2']):\n",
    "        diagnosis = item['diagnosis_1']\n",
    "    else:\n",
    "        diagnosis = item['diagnosis_2'] \n",
    "        \n",
    "    content = [{\"type\": \"text\",\"text\": prompt1 + f\"Input an image and the metadata concerning the patient:\\n approximate age: {metadata['age_approx']}, lesion location: {metadata['anatom_site_general']}, patient sex: {metadata['sex']}\"+system_prompt+\"\\n Be concise and short\"\n",
    "}]\n",
    "    content.append({\"type\":\"image\"})\n",
    "    output = {\n",
    "        \"image_id\":image_id,\n",
    "        \"prompt\":[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": content\n",
    "            }],\n",
    "        \"image\":[image.resize((224,224))],\n",
    "        \"answer\":diagnosis\n",
    "    }\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "dataset = FormattedDataset(dataset=df,format_fn=process_sample)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda8510a-e192-4fb4-8ef1-b4b5ddd4ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "solution_end_regex = r\"</answer>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "\n",
    "        # No need to reward <start_working_out> since we always prepend it!\n",
    "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    benin_labels = {\n",
    "        \"Benign melanocytic proliferations\",\n",
    "        \"Benign - Other\",\n",
    "        \"Benign epidermal proliferations\",\n",
    "        \"Benign soft tissue proliferations - Fibro-histiocytic\",\n",
    "        \"Benign soft tissue proliferations - Vascular\"\n",
    "    }\n",
    "    malin_labels = {\n",
    "        \"Malignant melanocytic proliferations (Melanoma)\",\n",
    "        \"Malignant adnexal epithelial proliferations - Follicular\",\n",
    "        \"Malignant epidermal proliferations\"\n",
    "    }\n",
    "\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "\n",
    "        if guess == true_answer:\n",
    "            score += 5.0\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        else:\n",
    "            # Bonus si guess et true_answer sont dans la mﾃｪme catﾃｩgorie (bﾃｩnin ou malin)\n",
    "            guess_benin = guess in benin_labels\n",
    "            guess_malin = guess in malin_labels\n",
    "            true_benin = true_answer in benin_labels\n",
    "            true_malin = true_answer in malin_labels\n",
    "\n",
    "            if (guess_benin and true_benin) or (guess_malin and true_malin):\n",
    "                score += 2.0\n",
    "            else:\n",
    "                # On essaye de calculer un ratio numﾃｩrique comme avant\n",
    "                try:\n",
    "                    ratio = float(guess) / float(true_answer)\n",
    "                    if 0.9 <= ratio <= 1.1:\n",
    "                        score += 2.0\n",
    "                    elif 0.8 <= ratio <= 1.2:\n",
    "                        score += 1.5\n",
    "                    else:\n",
    "                        score -= 2.5\n",
    "                except:\n",
    "                    score -= 4.5\n",
    "        scores.append(score)\n",
    "    return scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc430f-103e-48b3-8260-26c4e1c1083e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,514 | Num Epochs = 1 | Total steps = 2,628\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 14,024,704/7,000,000,000 (0.20% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0338\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.6481\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='179' max='2628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 179/2628 3:40:46 < 50:54:41, 0.01 it/s, Epoch 0.07/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / match_format_exactly</th>\n",
       "      <th>rewards / match_format_approximately</th>\n",
       "      <th>rewards / check_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>2.488351</td>\n",
       "      <td>115.687500</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>-2.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>134.312500</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.154701</td>\n",
       "      <td>115.562500</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>2.559401</td>\n",
       "      <td>138.187500</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.437500</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>122.500000</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>3.077106</td>\n",
       "      <td>135.625000</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.093750</td>\n",
       "      <td>3.272199</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>2.236621</td>\n",
       "      <td>164.187500</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.590727</td>\n",
       "      <td>159.687500</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>-2.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.939978</td>\n",
       "      <td>172.062500</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>-3.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>135.312500</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.750000</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>144.625000</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>-4.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>3.579785</td>\n",
       "      <td>148.687500</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-1.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>124.437500</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-2.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>179.250000</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>127.125000</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.288675</td>\n",
       "      <td>146.562500</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>118.437500</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>134.750000</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>139.625000</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>2.294920</td>\n",
       "      <td>136.250000</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>3.562170</td>\n",
       "      <td>159.500000</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>-1.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>3.532041</td>\n",
       "      <td>129.500000</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-1.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.045825</td>\n",
       "      <td>133.750000</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.875000</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.108678</td>\n",
       "      <td>142.875000</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.309401</td>\n",
       "      <td>120.375000</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.045825</td>\n",
       "      <td>130.062500</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>139.625000</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.954785</td>\n",
       "      <td>149.437500</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-2.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>132.562500</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.590727</td>\n",
       "      <td>142.062500</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>142.812500</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.406250</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>131.312500</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>3.349638</td>\n",
       "      <td>126.375000</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130.937500</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145.125000</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>142.250000</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>138.500000</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>134.250000</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>130.625000</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>1.204021</td>\n",
       "      <td>125.562500</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>124.500000</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.057587</td>\n",
       "      <td>145.125000</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>130.625000</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>1.967201</td>\n",
       "      <td>129.312500</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.154701</td>\n",
       "      <td>119.687500</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>1.327106</td>\n",
       "      <td>138.562500</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>140.625000</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.406250</td>\n",
       "      <td>2.057587</td>\n",
       "      <td>126.687500</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>136.562500</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>151.937500</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>132.812500</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>123.125000</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.812500</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>2.621207</td>\n",
       "      <td>140.062500</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.812500</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1.779701</td>\n",
       "      <td>127.875000</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>1.967201</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129.375000</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>159.500000</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>165.937500</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>146.187500</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>132.562500</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>124.687500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.812500</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>147.250000</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>143.312500</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>141.125000</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>124.875000</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>2.212287</td>\n",
       "      <td>141.750000</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.154701</td>\n",
       "      <td>144.250000</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>2.632250</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>-3.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>135.875000</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>1.967201</td>\n",
       "      <td>116.750000</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>2.897199</td>\n",
       "      <td>135.312500</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.968750</td>\n",
       "      <td>1.502478</td>\n",
       "      <td>137.250000</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>133.750000</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.093750</td>\n",
       "      <td>3.231806</td>\n",
       "      <td>132.500000</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.562500</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.154701</td>\n",
       "      <td>128.750000</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>140.312500</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134.187500</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>2.344356</td>\n",
       "      <td>122.625000</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>134.562500</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>2.057587</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.404701</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.906250</td>\n",
       "      <td>3.108722</td>\n",
       "      <td>128.250000</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>152.125000</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>149.375000</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.057587</td>\n",
       "      <td>118.187500</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>130.125000</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>2.342201</td>\n",
       "      <td>124.875000</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.636608</td>\n",
       "      <td>130.437500</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-1.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>128.937500</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134.250000</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.781250</td>\n",
       "      <td>2.932587</td>\n",
       "      <td>139.500000</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.391521</td>\n",
       "      <td>134.062500</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-2.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>144.437500</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-4.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>2.252193</td>\n",
       "      <td>137.312500</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.187500</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.349306</td>\n",
       "      <td>133.562500</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132.875000</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.093750</td>\n",
       "      <td>1.951304</td>\n",
       "      <td>118.500000</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.649787</td>\n",
       "      <td>146.687500</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-2.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.371207</td>\n",
       "      <td>123.437500</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.093750</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>128.375000</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.057587</td>\n",
       "      <td>131.812500</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>4.365093</td>\n",
       "      <td>147.250000</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>141.562500</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.281250</td>\n",
       "      <td>2.731897</td>\n",
       "      <td>145.687500</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>133.062500</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.889606</td>\n",
       "      <td>124.812500</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.406250</td>\n",
       "      <td>2.108722</td>\n",
       "      <td>123.312500</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129.937500</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>124.187500</td>\n",
       "      <td>0.004375</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>1.766521</td>\n",
       "      <td>140.312500</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>3.349304</td>\n",
       "      <td>135.625000</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>2.178695</td>\n",
       "      <td>147.125000</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>1.750694</td>\n",
       "      <td>146.062500</td>\n",
       "      <td>0.005760</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>2.313194</td>\n",
       "      <td>138.750000</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>2.685339</td>\n",
       "      <td>135.062500</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>138.875000</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>1.763041</td>\n",
       "      <td>108.937500</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>123.250000</td>\n",
       "      <td>0.012456</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>1.573817</td>\n",
       "      <td>112.062500</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>4.468750</td>\n",
       "      <td>2.077106</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>1.858325</td>\n",
       "      <td>122.687500</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.216506</td>\n",
       "      <td>127.937500</td>\n",
       "      <td>0.011460</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.968750</td>\n",
       "      <td>2.558707</td>\n",
       "      <td>135.062500</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>131.937500</td>\n",
       "      <td>0.010095</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>144.750000</td>\n",
       "      <td>0.014391</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>1.119780</td>\n",
       "      <td>116.125000</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>1.001951</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.485322</td>\n",
       "      <td>146.562500</td>\n",
       "      <td>0.016615</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>3.968750</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>0.015325</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>0.011741</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-1.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.906250</td>\n",
       "      <td>1.647199</td>\n",
       "      <td>137.500000</td>\n",
       "      <td>0.012620</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>130.562500</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.189451</td>\n",
       "      <td>139.375000</td>\n",
       "      <td>0.021563</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>2.418332</td>\n",
       "      <td>134.875000</td>\n",
       "      <td>0.018577</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>6.468750</td>\n",
       "      <td>2.871901</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>3.119780</td>\n",
       "      <td>136.062500</td>\n",
       "      <td>0.014498</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>2.938194</td>\n",
       "      <td>137.125000</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>2.000694</td>\n",
       "      <td>145.250000</td>\n",
       "      <td>0.016957</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.781250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>138.062500</td>\n",
       "      <td>0.019732</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>115.625000</td>\n",
       "      <td>0.020807</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.746207</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>3.049534</td>\n",
       "      <td>140.750000</td>\n",
       "      <td>0.024421</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>7.781250</td>\n",
       "      <td>1.734035</td>\n",
       "      <td>125.062500</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>1.563194</td>\n",
       "      <td>159.437500</td>\n",
       "      <td>0.017846</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-1.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>2.031250</td>\n",
       "      <td>2.688888</td>\n",
       "      <td>125.812500</td>\n",
       "      <td>0.022697</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>132.375000</td>\n",
       "      <td>0.016191</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>1.750694</td>\n",
       "      <td>138.312500</td>\n",
       "      <td>0.020022</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>2.544397</td>\n",
       "      <td>140.062500</td>\n",
       "      <td>0.018126</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>1.750694</td>\n",
       "      <td>137.937500</td>\n",
       "      <td>0.014860</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.371207</td>\n",
       "      <td>126.125000</td>\n",
       "      <td>0.023220</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>7.562500</td>\n",
       "      <td>1.688194</td>\n",
       "      <td>120.750000</td>\n",
       "      <td>0.023467</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>2.373158</td>\n",
       "      <td>120.125000</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>5.968750</td>\n",
       "      <td>2.933707</td>\n",
       "      <td>125.750000</td>\n",
       "      <td>0.020578</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.216506</td>\n",
       "      <td>120.687500</td>\n",
       "      <td>0.020845</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>2.189451</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>3.968750</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>105.312500</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-0.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>0.024581</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>129.562500</td>\n",
       "      <td>0.019367</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>2.307280</td>\n",
       "      <td>133.375000</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>5.656250</td>\n",
       "      <td>1.933707</td>\n",
       "      <td>120.062500</td>\n",
       "      <td>0.027110</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.156250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0530\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0632\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0609\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0610\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0001\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0316\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0255\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0558\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0001\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0372\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0526\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0621\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0335\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0696\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0278\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0502\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0213\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0321\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0401\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0452\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0493\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0659\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0691\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0705\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0001\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0269\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0336\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0442\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0442\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0400\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0400\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0325\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0325\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0280\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0300\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0527\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0454\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0514\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0559\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0685\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0363\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0605\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0361\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0361\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0322\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0483\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0484\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0484\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0377\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0376\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0305\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0469\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0516\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0596\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0460\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0460\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0323\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0323\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0303\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0553\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0327\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0327\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0421\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0614\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0631\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0789\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0330\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0376\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0582\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0348\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0348\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0346\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0365\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0368\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0489\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0350\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0350\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0342\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0370\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0370\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0351\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0352\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0377\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0515\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0664\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0792\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0488\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0301\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0301\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0397\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0566\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0375\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0375\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0433\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0559\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0365\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0365\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0468\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0657\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0339\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0655\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0351\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0588\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0339\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0316\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0480\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0306\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0306\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0325\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0550\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0416\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0356\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0641\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0326\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0268\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0548\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0369\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0368\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0011\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0403\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0466\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0466\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0464\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0563\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0637\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0008\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0012\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0498\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0644\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0477\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0646\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0457\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0576\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0436\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0436\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0339\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0652\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0010\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0017\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0403\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0613\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0014\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0469\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0014\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0030\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0005\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 2.3409\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0010\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0721\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0406\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0685\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0435\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0713\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0490\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0862\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0008\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0326\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0008\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0323\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0007\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0459\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0239\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0240\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0634\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0756\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0583\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0735\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0447\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0447\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0006\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0011\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0586\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0837\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0007\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0523\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0657\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0718\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0433\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0585\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0008\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0010\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0403\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0549\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0007\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0723\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0719\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0676\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0810\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0447\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0449\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0568\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0568\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0528\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0527\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0702\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0982\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0006\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0666\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0009\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0014\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0421\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0828\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0338\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0651\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0668\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0666\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0813\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0814\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0011\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0022\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0754\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0964\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0666\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0663\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0725\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0868\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0016\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0621\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0021\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0554\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0669\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0876\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0620\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0717\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0023\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0408\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0609\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0979\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0438\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0744\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0597\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1079\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0371\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0521\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0778\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0774\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0021\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0438\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0462\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0652\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0605\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0695\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0029\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0379\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0030\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0553\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0031\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0424\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0509\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0652\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0657\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0723\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0418\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0418\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0635\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0633\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0016\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0382\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0608\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0805\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0903\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0997\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0693\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1055\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0571\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0886\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0720\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0843\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0680\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0891\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0512\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0512\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0031\n",
      "[DEBUG] Loss: 0.0006\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0682\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0843\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1151\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0607\n",
      "[DEBUG] Loss: 0.0007\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1325\n",
      "[DEBUG] Loss: 0.0010\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0736\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1054\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0622\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0771\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0886\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1092\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0519\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0521\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0585\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0743\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0499\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0812\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0527\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0722\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0500\n",
      "[DEBUG] Loss: 0.0006\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0504\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0779\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1053\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0020\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0994\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0804\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1220\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0698\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0698\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0471\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0713\n",
      "[DEBUG] Loss: 0.0006\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0748\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1204\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0033\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0057\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0870\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0870\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0668\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0876\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0580\n",
      "[DEBUG] Loss: 0.0006\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1133\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0975\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0980\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from vlmgrpo import VLMGRPOTrainer\n",
    "from trl import GRPOConfig\n",
    "\n",
    "if hasattr(model, \"_flag_for_generation\"):\n",
    "    model._flag_for_generation = False\n",
    "FastVisionModel.for_training(model)\n",
    "    \n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-5,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    bf16 = is_bf16_supported(),\n",
    "    fp16 = not is_bf16_supported(),\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 2, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = 512,\n",
    "    max_completion_length = 256,\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    #max_steps = 250,\n",
    "    #save_steps = 250,\n",
    "    max_grad_norm = 0.3,\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = VLMGRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class=tokenizer, # MUST put unsloth processor here !\n",
    "    reward_processing_classes = tokenizer, #Here also\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        \n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "    grad_verbose=True\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072b557e-f1fa-477e-82a2-3e4c4ad7ff0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"V1_GRPO\")  # Local saving\n",
    "tokenizer.save_pretrained(\"V1_GRPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83338c5-50ec-42f9-b6d3-992944099386",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=df.iloc[int(len(df)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9a039-86c9-4f44-b057-0d22be0b26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "FastVisionModel.for_inference(model)\n",
    "acc=0\n",
    "import pickle\n",
    "data_list=[]\n",
    "thinking_pattern = f'{reasoning_start}(.*?){reasoning_end}'\n",
    "answer_pattern = f'{solution_start}(.*?){solution_end}'\n",
    "count=0\n",
    "quant=0\n",
    "FN=0\n",
    "FP=0\n",
    "TP=0\n",
    "TN=0\n",
    "benin_labels = {\n",
    "\"Benign melanocytic proliferations\",\n",
    "\"Benign - Other\",\n",
    "\"Benign epidermal proliferations\",\n",
    "\"Benign soft tissue proliferations - Fibro-histiocytic\",\n",
    "\"Benign soft tissue proliferations - Vascular\",\n",
    "\"Indeterminate epidermal proliferations\",\n",
    "\"Indeterminate melanocytic proliferations\"\n",
    "}\n",
    "malin_labels = {\n",
    "\"Malignant melanocytic proliferations (Melanoma)\",\n",
    "\"Malignant adnexal epithelial proliferations - Follicular\",\n",
    "\"Malignant epidermal proliferations\"\n",
    "}\n",
    "\n",
    "\n",
    "def contains_label(text, labels):\n",
    "    text_lower = text.lower()\n",
    "    return any(label.lower() in text_lower for label in labels)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    \n",
    "    proc = process_sample(test_dataset.iloc[i])\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\":\"text\",\"text\":proc[\"prompt\"][0][\"content\"]}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "        proc[\"image\"],\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            use_cache=True,\n",
    "            temperature=0.5,\n",
    "            min_p=0.1\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    if \"assistant\\n\" in decoded:\n",
    "        prediction_clean = decoded.split(\"assistant\\n\")[-1].strip()\n",
    "    else:\n",
    "        prediction_clean = decoded.strip()\n",
    "        \n",
    "    thinking_matches = re.findall(thinking_pattern, prediction_clean, re.DOTALL)\n",
    "    answer_matches = re.findall(answer_pattern, prediction_clean, re.DOTALL)\n",
    "    \n",
    "    if len(thinking_matches) == 1 and len(answer_matches) == 1:\n",
    "        guess_benin = contains_label(proc[\"answer\"], benin_labels)\n",
    "        guess_malin = contains_label(proc[\"answer\"], malin_labels)\n",
    "        true_benin = contains_label(answer_matches[0], benin_labels)\n",
    "        true_malin = contains_label(answer_matches[0], malin_labels)\n",
    "        \n",
    "        if true_malin or true_benin:\n",
    "            quant += 1\n",
    "            if (true_malin and guess_malin) :\n",
    "                TP += 1\n",
    "            if (true_benin and guess_benin) :\n",
    "                TN +=1\n",
    "            if true_malin and guess_benin:\n",
    "                FN += 1\n",
    "            if true_benin and guess_malin:\n",
    "                FP += 1\n",
    "                \n",
    "    print(\"accuracy:\", count/quant if quant > 0 else 0)\n",
    "    print(\"FN:\", FN,\"FP:\",FP,\"TN:\",TN,\"TP:\",TP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713bbcb-f7a7-45d1-bd68-5b5dc659f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10674e3b-4724-4092-ad7e-7d4736fb72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r best_weights.zip V2_GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747eac8-154d-4226-8f19-11ddb4c671c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
